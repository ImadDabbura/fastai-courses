{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic class handling tweaks of the training loop by changing a [Learner](https://dev.fast.ai/13a_learner#Learner) in various events.\n",
    "\n",
    "The training loop consists of a minimal set of instructions; looping through the data we:\n",
    "- compute the output of the model from the input\n",
    "- calculate a loss between this output and the desired target\n",
    "- compute the gradients of this loss with respect to all the model parameters\n",
    "- update the parameters accordingly\n",
    "- zero all the gradients\n",
    "\n",
    "Any tweak of this training loop is defined in a [Callback](https://dev.fast.ai/callback.core#Callback) to avoid over-complicating the code of the training loop, and to make it easy to mix and match different techniques (since they'll be defined in different callbacks). A callback can implement actions on the following events:\n",
    "1. `begin_fit`: called before doing anything, ideal for initial setup.\n",
    "1. `begin_epoch`: called at the beginning of each epoch, useful for any behavior you need to reset at each epoch.\n",
    "1. `begin_train`: called at the beginning of the training part of an epoch.\n",
    "1. `begin_batch`: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance).\n",
    "1. `after_pred`: called after computing the output of the model on the batch. It can be used to change that output before it's fed to the loss.\n",
    "1. `after_loss`: called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance).\n",
    "1. `after_backward`: called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance).\n",
    "1. `after_step`: called after the step and before the gradients are zeroed.\n",
    "1. `after_batch`: called at the end of a batch, for any clean-up before the next one.\n",
    "1. `after_train`: called at the end of the training phase of an epoch.\n",
    "1. `begin_validate`: called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation.\n",
    "1. `after_validate`: called at the end of the validation part of an epoch.\n",
    "1. `after_epoch`: called at the end of an epoch, for any clean-up before the next one.\n",
    "1. `after_fit`: called at the end of training, for final clean-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Callback():\n",
    "    _order = 0\n",
    "    def set_runner(self, run): \n",
    "        self.run = run\n",
    "\n",
    "    # This is used if we try to access a callback attribute\n",
    "    # but most likely the attribute is from runner.\n",
    "    def __getattr__(self, k):\n",
    "        return getattr(self.run, k)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        '''\n",
    "        Returns the name of the callback after removing the word `callback` \n",
    "        and then convert it to snake (split words by underscores).\n",
    "        '''\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')\n",
    "\n",
    "    # We have this method here so that the user has more flexibility\n",
    "    # about what to do when each callback is called. we can just change\n",
    "    # the behavior of the __call__ method\n",
    "    def __call__(self, cb_name):\n",
    "        f = getattr(self, cb_name, None)\n",
    "        if f and f():\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.run.n_epochs = 0.\n",
    "        self.run.n_iter = 0\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.run.in_train:\n",
    "            return\n",
    "        self.run.n_epochs += 1./self.iters\n",
    "        self.run.n_iter += 1\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs = self.epoch\n",
    "        self.model.train()\n",
    "        self.run.in_train = True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# These exceptions will be used to either:\n",
    "# 1. stop training\n",
    "# 2. skip to next epoch\n",
    "# 3. skip to next batch\n",
    "\n",
    "\n",
    "class CancelTrainException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CancelEpochException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CancelBatchException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop, self.cbs = False, [TrainEvalCallback()] + cbs\n",
    "\n",
    "    @property\n",
    "    def opt(self):\n",
    "        return self.learn.opt\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.learn.model\n",
    "\n",
    "    @property\n",
    "    def loss_func(self):\n",
    "        return self.learn.loss_func\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.learn.data\n",
    "\n",
    "    def one_batch(self, xb, yb):\n",
    "        try:\n",
    "            self.xb, self.yb = xb, yb\n",
    "            self('begin_batch')\n",
    "            self.pred = self.model(self.xb)\n",
    "            self('after_pred')\n",
    "            self.loss = self.loss_func(self.pred, self.yb)\n",
    "            self('after_loss')\n",
    "            if not self.in_train:\n",
    "                return\n",
    "            self.loss.backward()\n",
    "            self('after_backward')\n",
    "            self.opt.step()\n",
    "            self('after_step')\n",
    "            self.opt.zero_grad()\n",
    "        except CancelBatchException:\n",
    "            self('after_cancel_batch')\n",
    "        finally:\n",
    "            self('after_batch')\n",
    "\n",
    "    def all_batches(self, dl):\n",
    "        self.iters = len(dl)\n",
    "        try:\n",
    "            for xb, yb in dl:\n",
    "                self.one_batch(xb, yb)\n",
    "        except CancelEpochException:\n",
    "            self('after_cancel_epoch')\n",
    "\n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs, self.learn, self.loss = epochs, learn, tensor(0.)\n",
    "\n",
    "        try:\n",
    "            for cb in self.cbs:\n",
    "                cb.set_runner(self)\n",
    "            self('begin_fit')\n",
    "            for epoch in range(epochs):\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'):\n",
    "                    self.all_batches(self.data.train_dl)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if not self('begin_validate'):\n",
    "                        self.all_batches(self.data.valid_dl)\n",
    "                self('after_epoch')\n",
    "\n",
    "        except CancelTrainException:\n",
    "            self('after_cancel_train')\n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        res = False\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            res = cb(cb_name) and res\n",
    "            if res:\n",
    "                print(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    _order = 1\n",
    "\n",
    "    def after_step(self):\n",
    "        print(self.n_iter)\n",
    "        if self.n_iter >= 10:\n",
    "            raise CancelTrainException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats, self.valid_stats = AvgStats(\n",
    "            metrics, True), AvgStats(metrics, False)\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "\n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad():\n",
    "            stats.accumulate(self.run)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        print(self.train_stats)\n",
    "        print(self.valid_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.lrs = [[] for _ in self.opt.param_groups]\n",
    "        self.losses = []\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train:\n",
    "            return\n",
    "        for pg, lr in zip(self.opt.param_groups, self.lrs):\n",
    "            lr.append(pg['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())\n",
    "\n",
    "    def plot_lr(self, pgid=-1):\n",
    "        plt.plot(self.lrs[pgid])\n",
    "\n",
    "    def plot_loss(self, skip_last=0):\n",
    "        plt.plot(self.losses[:len(self.losses) - skip_last])\n",
    "\n",
    "    def plot(self, skip_last=0, pgid=-1):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        lrs = self.lrs[pgid]\n",
    "        n = len(losses) - skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(lrs[:n], losses[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ParamScheduler(Callback):\n",
    "    _order = 1\n",
    "\n",
    "    def __init__(self, pname, sched_funcs):\n",
    "        self.pname, self.sched_funcs = pname, sched_funcs\n",
    "\n",
    "    def begin_fit(self):\n",
    "        if not isinstance(self.sched_funcs, (list, tuple)):\n",
    "            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n",
    "\n",
    "    def set_param(self):\n",
    "        assert len(self.opt.param_groups) == len(self.sched_funcs)\n",
    "        for pg, f in zip(self.opt.param_groups, self.sched_funcs):\n",
    "            pg[self.pname] = f(self.n_epochs / self.epochs)\n",
    "\n",
    "    def begin_batch(self):\n",
    "        if self.in_train:\n",
    "            self.set_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LR_Find(Callback):\n",
    "    _order = 1\n",
    "\n",
    "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter, self.min_lr, self.max_lr = max_iter, min_lr, max_lr\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def begin_batch(self):\n",
    "        if not self.in_train:\n",
    "            return\n",
    "        pos = self.n_iter / self.max_iter\n",
    "        lr = self.min_lr * (self.max_lr / self.min_lr)**pos\n",
    "        for pg in self.opt.param_groups:\n",
    "            pg['lr'] = lr\n",
    "\n",
    "    def after_step(self):\n",
    "        if self.n_iter >= self.max_iter or self.loss > self.best_loss * 10:\n",
    "            raise CancelTrainException()\n",
    "        if self.loss < self.best_loss:\n",
    "            self.best_loss = self.loss"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
