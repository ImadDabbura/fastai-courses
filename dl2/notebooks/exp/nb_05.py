
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/05_Anneal-Imad.ipynb

from exp.nb_04 import *
import functools

def create_learner(model_func, loss_func, data):
    return Learner(*model_func(data), loss_func, data)

def get_model_func(lr=0.5):
    return partial(get_model, lr=lr)

class Recorder(Callback):
    def begin_fit(self):
        self.lrs = []
        self.losses = []

    def after_batch(self):
        if not self.in_train:
            return
        # Append the learning rate of the last parameter groups
        self.lrs.append(self.opt.param_groups[-1]['lr'])
        self.losses.append(self.loss.detach().cpu())

    def plot_lr(self):
        plt.plot(self.lrs)

    def plot_loss(self):
        plt.plot(self.losses)


class ParamScheduler(Callback):
    _order = 1

    def __init__(self, pname, sched_func):
        self.pname = pname
        self.sched_func = sched_func

    def begin_batch(self):
        if self.in_train:
            for pg in self.opt.param_groups:
                pg[self.pname] = self.sched_func(self.n_epochs / self.epochs)

def annealer(func):
    functools.wraps(func)

    def annealer_wrapper(*args, **kwargs):
        return functools.partial(func, *args)
    return annealer_wrapper


@annealer
def lin_sched(start, end, pos):
    return start + pos * (end - start)

@annealer
def sched_cos(start, end, pos):
    return start + (1 + math.cos(math.pi * (1 - pos))) * (end - start) / 2


@annealer
def sched_no(start, end, pos):
    return start


@annealer
def sched_exp(start, end, pos):
    return start * (end / start)**pos

def combine_scheds(pcts, scheds):
    """Combine different scheduler of hyper-parameters during training."""
    assert sum(pcts) == 1.
    pcts = torch.tensor([0] + listify(pcts))
    assert torch.all(pcts >= 0)
    pcts = torch.cumsum(pcts, 0)
    def _inner(pos):
        # Determine which scheduler to use
        idx = (pos >= pcts).nonzero().max()
        # Determine the actual position to be used by the chosen scheduler
        actual_pos = (pos - pcts[idx]) / (pcts[idx + 1] - pcts[idx])
        return scheds[idx](actual_pos)

    return _inner