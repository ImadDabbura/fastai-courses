{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imad/anaconda3/envs/dl/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from exp.nb_11a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the IMDB dataset that consists of 50,000 labeled reviews of movies (positive or negative) and 50,000 unlabelled ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=4964)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [144441344/144440600 00:34&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('/Users/imad/.fastai/data/imdb/test'),\n",
       " Path('/Users/imad/.fastai/data/imdb/tmp_clas'),\n",
       " Path('/Users/imad/.fastai/data/imdb/imdb.vocab'),\n",
       " Path('/Users/imad/.fastai/data/imdb/unsup'),\n",
       " Path('/Users/imad/.fastai/data/imdb/README'),\n",
       " Path('/Users/imad/.fastai/data/imdb/tmp_lm'),\n",
       " Path('/Users/imad/.fastai/data/imdb/train')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a subclass of `ItemList` that will read the texts in the corresponding filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_file(fn):\n",
    "    with open(fn, \"r\", encoding=\"utf8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "class TextList(ItemList):\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions=\".txt\", recurse=True, include=None, **kwargs):\n",
    "        return cls(\n",
    "            get_files(path, extensions, recurse=recurse, include=include),\n",
    "            path,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def get(self, i):\n",
    "        if isinstance(i, Path):\n",
    "            return read_file(i)\n",
    "        return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case there are some text log files, we restrict the ones we take to the training, test, and unsupervised folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=[\"train\", \"test\", \"unsup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextList (100000 items)\n",
       "[Path('/Users/imad/.fastai/data/imdb/test/neg/1821_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/9487_1.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/4604_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/2828_2.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/10890_1.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/3351_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/8070_2.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/1027_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/8248_3.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/4290_4.txt')...]\n",
       "Path: /Users/imad/.fastai/data/imdb"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "il"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should expect a total of 100,000 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(il.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first one as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = il[0]\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text classification, we will split by the grand parent folder as before, but for language modeling, we take all the texts and just put 10% aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: TextList (89934 items)\n",
       "[Path('/Users/imad/.fastai/data/imdb/test/neg/1821_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/9487_1.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/4604_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/2828_2.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/10890_1.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/3351_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/8070_2.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/8248_3.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/4290_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/10096_1.txt')...]\n",
       "Path: /Users/imad/.fastai/data/imdb\n",
       "Valid: TextList (10066 items)\n",
       "[Path('/Users/imad/.fastai/data/imdb/test/neg/1027_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/3760_1.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/6900_1.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/3583_3.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/2813_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/9149_3.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/5228_4.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/1162_1.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/1127_1.txt'), Path('/Users/imad/.fastai/data/imdb/test/neg/6006_4.txt')...]\n",
       "Path: /Users/imad/.fastai/data/imdb"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mrandom_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mrandom_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mp_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Documents/courses/fastai-courses/dl2/notebooks/exp/nb_11a.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_splitter??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize the dataset first, which is splitting a sentence in individual tokens. Those tokens are the basic words or punctuation signs with a few tweaks: don't for instance is split between do and n't. We will use a processor for this, in conjunction with the [spacy library](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=5070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before even tokenizeing, we will apply a bit of preprocessing on the texts to clean them up (we saw the one up there had some HTML code). These rules are applied before we split the sentences in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# special tokens\n",
    "(\n",
    "    UNK,\n",
    "    PAD,\n",
    "    BOS,\n",
    "    EOS,\n",
    "    TK_REP,\n",
    "    TK_WREP,\n",
    "    TK_UP,\n",
    "    TK_MAJ,\n",
    ") = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "\n",
    "def sub_br(t):\n",
    "    \"Replaces the <br /> by \\n\"\n",
    "    re_br = re.compile(r\"<\\s*br\\s*/?>\", re.IGNORECASE)\n",
    "    return re_br.sub(\"\\n\", t)\n",
    "\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return re.sub(r\"([/#])\", r\" \\1 \", t)\n",
    "\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return re.sub(\" {2,}\", \" \", t)\n",
    "\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "\n",
    "    def _replace_rep(m: Collection[str]) -> str:\n",
    "        c, cc = m.groups()\n",
    "        return f\" {TK_REP} {len(cc)+1} {c} \"\n",
    "\n",
    "    re_rep = re.compile(r\"(\\S)(\\1{3,})\")\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "\n",
    "\n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word -> TK_WREP 3 word\"\n",
    "\n",
    "    def _replace_wrep(m: Collection[str]) -> str:\n",
    "        c, cc = m.groups()\n",
    "        return f\" {TK_WREP} {len(cc.split())+1} {c} \"\n",
    "\n",
    "    re_wrep = re.compile(r\"(\\b\\w+\\W+)(\\1{3,})\")\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "\n",
    "def fixup_text(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    re1 = re.compile(r\"  +\")\n",
    "    x = (\n",
    "        x.replace(\"#39;\", \"'\")\n",
    "        .replace(\"amp;\", \"&\")\n",
    "        .replace(\"#146;\", \"'\")\n",
    "        .replace(\"nbsp;\", \" \")\n",
    "        .replace(\"#36;\", \"$\")\n",
    "        .replace(\"\\\\n\", \"\\n\")\n",
    "        .replace(\"quot;\", \"'\")\n",
    "        .replace(\"<br />\", \"\\n\")\n",
    "        .replace('\\\\\"', '\"')\n",
    "        .replace(\"<unk>\", UNK)\n",
    "        .replace(\" @.@ \", \".\")\n",
    "        .replace(\" @-@ \", \"-\")\n",
    "        .replace(\"\\\\\", \" \\\\ \")\n",
    "    )\n",
    "    return re1.sub(\" \", html.unescape(x))\n",
    "\n",
    "\n",
    "default_pre_rules = [\n",
    "    fixup_text,\n",
    "    replace_rep,\n",
    "    replace_wrep,\n",
    "    spec_add_spaces,\n",
    "    rm_useless_spaces,\n",
    "    sub_br,\n",
    "]\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxrep 4 c '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_rep(\"cccc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxwrep 5 word  '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_wrep(\"word word word word word \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rules are applies after the tokenization on the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def replace_all_caps(x):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t.isupper() and len(t) > 1:\n",
    "            res.append(TK_UP)\n",
    "            res.append(t.lower())\n",
    "        else:\n",
    "            res.append(t)\n",
    "    return res\n",
    "\n",
    "\n",
    "def deal_caps(x):\n",
    "    \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t == \"\":\n",
    "            continue\n",
    "        if t[0].isupper() and len(t) > 1 and t[1:].islower():\n",
    "            res.append(TK_MAJ)\n",
    "        res.append(t.lower())\n",
    "    return res\n",
    "\n",
    "\n",
    "def add_eos_bos(x):\n",
    "    return [BOS] + x + [EOS]\n",
    "\n",
    "\n",
    "default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'xxup', 'am', 'xxup', 'shouting']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_all_caps([\"I\", \"AM\", \"SHOUTING\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxmaj', 'my', 'name', 'is', 'xxmaj', 'jeremy']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_caps([\"My\", \"name\", \"is\", \"Jeremy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tokenizing and applying those rules takes a bit of time, we'll parallelize it using `ProcessPoolExecutor` to go faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from spacy.symbols import ORTH\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "def parallel(func, arr, max_workers=4):\n",
    "    if max_workers < 2:\n",
    "        results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))\n",
    "    else:\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))\n",
    "    if any([o is not None for o in results]):\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TokenizeProcessor(Processor):\n",
    "    def __init__(\n",
    "        self, lang=\"en\", chunksize=2000, pre_rules=None, post_rules=None, max_workers=4\n",
    "    ):\n",
    "        self.chunksize, self.max_workers = chunksize, max_workers\n",
    "        self.tokenizer = spacy.blank(lang).tokenizer\n",
    "        for w in default_spec_tok:\n",
    "            self.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.pre_rules = default_pre_rules if pre_rules is None else pre_rules\n",
    "        self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "\n",
    "    def proc_chunk(self, args):\n",
    "        i, chunk = args\n",
    "        chunk = [compose(t, self.pre_rules) for t in chunk]\n",
    "        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]\n",
    "        docs = [compose(t, self.post_rules) for t in docs]\n",
    "        return docs\n",
    "\n",
    "    def __call__(self, items):\n",
    "        toks = []\n",
    "        if isinstance(items[0], Path):\n",
    "            items = [read_file(i) for i in items]\n",
    "        chunks = [\n",
    "            items[i : i + self.chunksize]\n",
    "            for i in (range(0, len(items), self.chunksize))\n",
    "        ]\n",
    "        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "        return sum(toks, [])\n",
    "\n",
    "    def proc1(self, item):\n",
    "        return self.proc_chunk([item])[0]\n",
    "\n",
    "    def deprocess(self, toks):\n",
    "        return [self.deproc1(tok) for tok in toks]\n",
    "\n",
    "    def deproc1(self, tok):\n",
    "        return \" \".join(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TokenizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the mo\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"xxbos • xxmaj • alan • xxmaj • rickman • & • xxmaj • emma • xxmaj • thompson • give • good • performances • with • southern • / • xxmaj • new • xxmaj • orleans • accents • in • this • detective • flick • . • xxmaj • it • 's • worth • seeing • for • their • scenes- • and • xxmaj • rickman • 's • scene • with • xxmaj • hal • xxmaj • holbrook • . • xxmaj • these • three • actors • mannage • to • ente\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" • \".join(tp(il[:100])[0])[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Numericalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have tokenized our texts, we replace each token by an individual number, this is called numericalizing. Again, we do this with a processor (not so different from the `CategoryProcessor`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=5491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import collections\n",
    "\n",
    "\n",
    "class NumericalizeProcessor(Processor):\n",
    "    def __init__(self, vocab=None, max_vocab=60000, min_freq=2):\n",
    "        self.vocab, self.max_vocab, self.min_freq = vocab, max_vocab, min_freq\n",
    "\n",
    "    def __call__(self, items):\n",
    "        # The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            freq = Counter(p for o in items for p in o)\n",
    "            self.vocab = [\n",
    "                o for o, c in freq.most_common(self.max_vocab) if c >= self.min_freq\n",
    "            ]\n",
    "            for o in reversed(default_spec_tok):\n",
    "                if o in self.vocab:\n",
    "                    self.vocab.remove(o)\n",
    "                self.vocab.insert(0, o)\n",
    "        if getattr(self, \"otoi\", None) is None:\n",
    "            self.otoi = collections.defaultdict(\n",
    "                int, {v: k for k, v in enumerate(self.vocab)}\n",
    "            )\n",
    "        return [self.proc1(o) for o in items]\n",
    "\n",
    "    def proc1(self, item):\n",
    "        return [self.otoi[o] for o in item]\n",
    "\n",
    "    def _deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "\n",
    "    def deproc1(self, idx):\n",
    "        return [self.vocab[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do language modeling, we will infer the labels from the text during training, so there's no need to label. The training loop expects labels however, so we need to add dummy ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok, proc_num = TokenizeProcessor(max_workers=8), NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='45' class='' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [45/45 00:38<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6/6 00:05<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 8.79 s, total: 27 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%time ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the items have been processed they will become list of numbers, we can still access the underlying raw data in `x_obj` (or `y_obj` for the targets, but we don't have any here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-3eef16a74d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/courses/fastai-courses/dl2/notebooks/exp/nb_08.py\u001b[0m in \u001b[0;36mx_obj\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mx_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0my_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/courses/fastai-courses/dl2/notebooks/exp/nb_08.py\u001b[0m in \u001b[0;36mobj\u001b[0;34m(self, items, idx, procs)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misint\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-69c90d86de60>\u001b[0m in \u001b[0;36m_deprocess\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_deprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-69c90d86de60>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_deprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-69c90d86de60>\u001b[0m in \u001b[0;36mdeproc1\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "ll.train.x_obj(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the preprocessing tajes time, we save the intermediate result using pickle. Don't use any lambda functions in your processors or they won't be able to pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ld.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ld.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bit of work to convert our `LabelList` in a `DataBunch` as we don't just want batches of IMDB reviews. We want to stream through all the texts concatenated. We also have to prepare the targets that are the newt words in the text. All of this is done with the next object called `LM_PreLoader`. At the beginning of each epoch, it'll shuffle the articles (if `shuffle=True`) and create a big stream by concatenating all of them. We divide this big stream in `bs` smaller streams. That we will read in chunks of bptt length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=5565)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just using those for illustration purposes, they're not used otherwise.\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say our stream is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = \"\"\"\n",
    "In this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. \n",
    "First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the Processor used in the data block API.\n",
    "Then we will study how we build a language model and train it.\\n\n",
    "\"\"\"\n",
    "tokens = np.array(tp([stream])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['xxbos', '\\n', 'xxmaj', 'in', 'this', 'notebook', ',', 'we',\n",
       "       'will', 'go', 'back', 'over', 'the', 'example', 'of',\n",
       "       'classifying', 'movie', 'reviews', 'we', 'studied', 'in', 'part',\n",
       "       '1', 'and', 'dig', 'deeper', 'under', 'the', 'surface', '.', '\\n',\n",
       "       'xxmaj', 'first', 'we', 'will', 'look', 'at', 'the', 'processing',\n",
       "       'steps', 'necessary', 'to', 'convert', 'text', 'into', 'numbers',\n",
       "       'and', 'how', 'to', 'customize', 'it', '.', 'xxmaj', 'by', 'doing',\n",
       "       'this', ',', 'we', \"'ll\", 'have', 'another', 'example', 'of',\n",
       "       'the', 'xxmaj', 'processor', 'used', 'in', 'the', 'data', 'block',\n",
       "       'api', '.', '\\n', 'xxmaj', 'then', 'we', 'will', 'study', 'how',\n",
       "       'we', 'build', 'a', 'language', 'model', 'and', 'train', 'it', '.',\n",
       "       '\\n\\n', 'xxeos'], dtype='<U11')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we split it in 6 batches it would give something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs, seq_len = 6, 15\n",
    "d_tokens = np.array([tokens[i * seq_len : (i + 1) * seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False, header=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we have a `bptt` of 5, we would go over those three batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs, bptt = 6, 5\n",
    "for k in range(3):\n",
    "    d_tokens = np.array(\n",
    "        [\n",
    "            tokens[i * seq_len + k * bptt : i * seq_len + (k + 1) * bptt]\n",
    "            for i in range(bs)\n",
    "        ]\n",
    "    )\n",
    "    df = pd.DataFrame(d_tokens)\n",
    "    display(HTML(df.to_html(index=False, header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LM_PreLoader:\n",
    "    def __init__(self, data, bs=64, bptt=70, shuffle=False):\n",
    "        self.data, self.bs, self.bptt, self.shuffle = data, bs, bptt, shuffle\n",
    "        total_len = sum([len(t) for t in data.x])  # number of tokens\n",
    "        self.n_batch = total_len // bs\n",
    "        self.batchify()\n",
    "\n",
    "    def __len__(self):\n",
    "        return ((self.n_batch - 1) // self.bptt) * self.bs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.batched_data[idx % self.bs]\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        return (\n",
    "            source[seq_idx : seq_idx + self.bptt],  # x\n",
    "            source[seq_idx + 1 : seq_idx + self.bptt + 1],  # y\n",
    "        )\n",
    "\n",
    "    def batchify(self):\n",
    "        texts = self.data.x\n",
    "        if self.shuffle:\n",
    "            texts = texts[torch.randperm(len(texts))]\n",
    "        stream = torch.cat([tensor(t) for t in texts])\n",
    "        print(stream.shape)\n",
    "        self.batched_data = stream[: self.n_batch * self.bs].view(self.bs, self.n_batch)\n",
    "        print(self.batched_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3018571])\n",
      "torch.Size([64, 47165])\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(LM_PreLoader(ll.valid, shuffle=True), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(673, 64, 70, 3018560, 47165, 10066)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\n",
    "    dl\n",
    "), dl.dataset.bs, dl.dataset.bptt, dl.dataset.n_batch * dl.dataset.bs, dl.dataset.n_batch, len(\n",
    "    dl.dataset.data.x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledData\n",
       "x: TextList (10066 items)\n",
       "[[2, 7, 2906, 11181, 36, 27, 15596, 21, 4982, 21, 222, 0, 14331, 2502, 10, 21, 7, 2186, 21, 3743, 12, 189, 13, 4606, 14, 874, 163, 11, 105, 262, 489, 141, 110, 805, 133, 30, 8, 426, 2928, 2670, 315, 11, 7, 936, 7, 18202, 10, 49, 10646, 40, 664, 17, 240, 190, 150, 10, 104, 427, 48, 1288, 1128, 257, 9, 12, 17608, 10, 14, 43, 274, 10, 30, 8, 69, 5166, 21, 7, 1226, 21, 10, 112, 127, 172, 329, 10, 15, 12, 3847, 1517, 947, 9, 18, 1484, 32, 126, 20, 319, 9, 36, 194, 377, 124, 256, 33, 3], [2, 7, 14, 662, 21, 7, 28369, 1717, 23, 2239, 21, 12, 1076, 1099, 391, 15, 706, 69, 14, 113, 762, 1315, 36, 2946, 33, 10, 113, 10714, 1247, 14, 11661, 34, 8, 5523, 28, 21, 7, 6462, 7, 1923, 21, 10, 11, 8, 1254, 13, 173, 49, 73, 329, 343, 17, 725, 26, 7, 1597, 7, 2527, 22, 7, 1497, 7, 3842, 10, 7, 18736, 22, 1268, 11, 7, 10755, 7, 11134, 9, 24, 7, 8, 29, 413, 15, 96, 215, 14, 43, 67, 6276, 28, 4017, 2574, 11, 15, 138, 2291, 27, 1634, 51, 7, 4929, 11, 8, 3898, 9, 7, 19, 15, 8, 555, 13, 29, 138, 4287, 14, 1294, 23518, 51, 8, 4017, 2574, 121, 133, 158, 5357, 10, 587, 23, 1756, 10, 866, 3638, 10, 11, 390, 13, 2254, 96, 137, 9, 7, 152, 3361, 174, 540, 53, 21, 7, 1281, 836, 7, 51, 7, 3129, 7, 760, 21, 58, 35, 7025, 529, 14, 5696, 1634, 36, 107, 8, 29, 1366, 50813, 8, 8813, 33, 10, 84, 96, 11, 782, 118, 53, 21, 7, 28369, 1717, 23, 2239, 21, 38, 4197, 9, 7, 52, 726, 65, 424, 14, 8, 717, 63, 32, 38, 169, 8, 4017, 2574, 350, 9, 24, 7, 8, 1167, 81, 61, 8, 883, 653, 48, 28220, 227, 489, 45, 8, 1923, 11, 289, 72, 34, 7, 5165, 9, 7, 64, 46, 186, 34, 20, 1372, 38, 8, 17900, 13, 12, 428, 23, 53, 4402, 10462, 47, 48, 7351, 314, 9, 7, 82, 42, 7, 11315, 15, 636, 17, 517, 23, 72, 10, 12, 1157, 278, 267, 49, 15, 2031, 55, 45, 242, 61, 74, 16143, 17, 56, 551, 9, 7, 8, 347, 185, 53, 8, 21, 10059, 21, 17, 8, 176, 7, 13072, 2205, 10, 46, 45906, 0, 208, 8, 19495, 11, 1367, 28447, 45, 8, 883, 27, 520, 4979, 23, 282, 63, 46, 38, 456, 14, 43, 2031, 9, 7, 13, 285, 618, 13, 19, 15, 144, 1835, 26, 422, 52, 73, 4816, 65, 1986, 13, 3510, 5483, 51, 8, 945, 13, 8, 923, 9, 24, 7, 8, 156, 34, 7, 5165, 38, 1408, 17, 158, 457, 21, 7, 14858, 7, 1433, 21, 14, 9386, 111, 51, 8, 396, 13, 8, 9969, 29, 9, 7, 63, 19, 61, 32, 549, 21, 7, 4740, 13, 7, 4205, 21, 32, 105, 43, 694, 107, 16, 15, 57, 332, 11, 482, 540, 27, 12, 3123, 1724, 21322, 1294, 14, 8, 2634, 17, 1220, 23, 389, 9, 24, 7, 17, 1827, 27, 8, 5063, 12679, 13, 8, 29, 10, 8, 11054, 651, 1642, 12682, 72, 56, 7416, 37830, 23, 214, 568, 11, 182, 9, 7, 56, 2070, 14, 3823, 72, 14, 8, 347, 2768, 56, 7515, 505, 14, 2292, 11, 71, 11, 7, 1497, 7, 3842, 34829, 15677, 14, 273, 102, 26, 46, 410, 79, 4275, 36, 7821, 496, 13, 25811, 149, 33, 9, 24, 7, 8, 164, 424, 13, 8, 29, 38, 8, 9473, 45, 8, 4103, 2197, 9, 7, 52, 94, 52, 20, 4017, 2574, 25, 1452, 14, 5297, 12344, 151, 30917, 1500, 9, 7, 8, 13914, 4598, 13, 21, 0, 21, 38, 2358, 28, 8, 106, 184, 248, 13, 8, 29, 10, 115, 213, 7605, 59, 1662, 12977, 539, 8, 19095, 11, 25966, 4331, 17, 8, 4103, 2197, 9, 7, 329, 46, 38, 457, 718, 14, 1019, 8, 5063, 1160, 930, 47, 8, 1730, 170, 14, 37889, 34, 8, 29, 22, 497, 117, 598, 763, 9, 24, 7, 8, 288, 61, 48, 580, 381, 3934, 648, 10, 15, 37, 36108, 10, 11, 15, 4367, 47, 12, 1577, 9, 7, 284, 32, 105, 43, 8888, 20, 16, 15, 82, 12693, 248, 10, 16, 15, 153, 59, 1960, 248, 117, 217, 26, 120, 1040, 794, 441, 13, 21, 7, 8, 7, 4502, 7, 3105, 21, 61, 461, 237, 69, 1531, 93, 19, 462, 29, 9, 24, 7, 115, 193, 10, 64, 58, 18, 140, 66, 18, 167, 82, 12, 518, 9, 3], [2, 7, 68, 77, 1419, 23, 317, 23, 176, 1608, 2464, 45, 8, 800, 320, 19, 29, 10, 20, 25, 44, 8, 23805, 18, 930, 20, 19, 31, 15, 12, 4031, 9, 7, 16, 25, 369, 9, 7, 8, 669, 528, 86, 369, 9, 7, 8, 11041, 528, 86, 369, 9, 7, 76, 8, 174, 528, 188, 27, 127, 1311, 5426, 188, 86, 369, 9, 7, 11, 741, 9, 7, 11, 88, 18, 772, 741, 66, 7, 2623, 99, 377, 33, 7, 8, 9962, 30, 1759, 7, 1268, 15473, 40, 6013, 13, 12, 537, 9, 256, 33, 7, 8, 9962, 30, 1759, 7, 1268, 2715, 8, 2948, 36, 152, 1262, 12, 634, 66, 50, 33, 9, 361, 33, 7, 8, 6013, 13, 12, 537, 7012, 14, 614, 11, 508, 10, 30, 10, 1199, 14, 40, 378, 10, 5769, 200, 60, 107, 46, 382, 108, 9, 224, 33, 7, 8, 6013, 13, 12, 537, 1761, 467, 1140, 3982, 9, 372, 33, 7, 11, 12, 8913, 123, 4779, 47, 48, 7, 2125, 23, 7, 297, 554, 9, 7, 207, 3196, 1106, 151, 1037, 28, 20, 245, 13, 2714, 11, 28, 26262, 20, 3585, 9, 18, 88, 772, 127, 174, 1720, 9, 7, 17, 214, 3095, 10, 7075, 47, 1311, 121, 9, 7, 8, 3431, 10577, 34, 8, 5516, 22, 8665, 25, 174, 9, 7, 8, 2948, 799, 1483, 1373, 14, 40, 21610, 6251, 152, 19827, 28, 11294, 25, 174, 9, 7, 30, 8, 396, 66, 7, 1360, 9, 7, 307, 796, 14, 114, 12, 7, 33574, 23, 7, 4889, 23, 7, 11436, 29, 11, 114, 16, 76, 32270, 47, 253, 16, 12, 1325, 23, 1319, 31, 10, 11, 19, 25, 8, 941, 9, 7, 11, 155, 114, 8, 30652, 7, 1748, 23, 1017, 994, 47, 0, 27, 79, 1456, 10, 270, 93, 57, 1901, 111, 66, 7, 453, 1161, 9, 7, 74, 558, 930, 17, 19, 31, 28, 379, 10, 497, 12825, 34, 6295, 4511, 13412, 10, 6620, 1931, 11, 13816, 465, 9, 7, 11, 8, 7226, 168, 23, 17, 23, 12, 23, 887, 66, 7, 57, 64, 628, 25, 16, 66, 12, 27543, 66, 7, 34192, 7, 1798, 66, 7, 64, 10, 1399, 12, 84, 393, 130, 2544, 14, 366, 7, 21500, 8, 7, 5749, 1301, 8, 7, 400, 13, 7, 44, 7, 3486, 66, 7, 22173, 15, 12, 1976, 9, 7, 11, 18, 88, 35, 76, 382, 14, 772, 8, 36819, 9, 3], [2, 7, 8, 241, 29, 25, 244, 334, 23, 15577, 9, 7, 16, 95, 41, 98, 12, 94, 146, 29, 30, 10, 20, 73, 41, 2590, 12, 865, 23, 951, 11, 287, 173, 9, 7, 1115, 14, 21, 7, 4857, 21, 19, 25, 12, 3695, 9, 18, 258, 57, 1297, 18, 88, 35, 41, 14, 964, 28, 16, 9, 24, 7, 1307, 99, 24, 7, 64, 25, 8, 243, 13, 280, 6477, 1623, 403, 3719, 66, 7, 14, 1547, 8, 3276, 13, 1164, 7748, 11, 114, 111, 250, 146, 66, 7, 34736, 23, 0, 50, 7, 8, 723, 86, 9655, 10, 2085, 47, 4848, 10, 11, 88, 35, 76, 1235, 8, 5291, 46, 86, 422, 133, 26, 3758, 60, 47, 7, 1758, 23, 7182, 24, 256, 60, 13, 184, 3], [2, 7, 219, 33928, 515, 14, 406, 2465, 222, 8, 14232, 11, 79, 806, 960, 9, 24, 7, 8388, 36, 48, 3889, 7, 1230, 7, 6909, 33, 61, 14, 370, 11, 1871, 14, 8, 720, 7, 1300, 2397, 36, 7, 5872, 7, 3755, 10, 7, 2246, 10, 3356, 14, 43, 1947, 10, 26, 682, 33, 20, 40, 474, 25, 530, 47, 8, 4379, 22, 1004, 3305, 49, 61, 104, 98, 1973, 17, 7136, 23, 16929, 68, 1260, 22, 278, 9, 7, 53522, 7, 209, 3457, 10, 171, 39, 1741, 40, 7136, 53, 16, 22, 12, 24812, 9, 24, 604, 891, 23, 1149, 534, 47, 127, 460, 10, 1079, 13, 926, 61, 128, 913, 34, 40, 6669, 10, 30, 19, 506, 2402, 35, 117, 96, 11, 16, 61, 12, 358, 365, 654, 75, 13, 15627, 248, 9, 3], [2, 7, 53, 8, 978, 13, 8, 2334, 10, 7, 3364, 7, 11471, 22, 1544, 1829, 11, 12164, 308, 35063, 143, 8, 31, 10, 30, 3055, 46, 38, 8, 82, 127, 200, 296, 169, 19, 31, 28, 9, 7, 68, 17, 3006, 89, 154, 43, 169, 28, 8, 1414, 9, 24, 7, 8, 130, 15, 1602, 34, 8, 1511, 10, 11, 268, 14, 283, 9, 7, 26, 339, 26, 7, 3364, 7, 11471, 15, 10, 71, 91, 35, 726, 120, 13, 8, 5510, 20, 71, 10340, 103, 8, 3085, 62, 7, 481, 7, 2373, 62, 226, 9, 7, 254, 7, 481, 7, 2373, 16, 210, 20, 7, 11471, 61, 98, 3933, 14, 2793, 766, 0, 64, 766, 1882, 50, 33, 10, 11, 15, 8, 4007, 313, 28, 1515, 56, 103, 125, 9, 24, 7, 8, 1100, 13, 8, 2942, 12245, 15, 9679, 11, 3993, 10, 27, 139, 123, 980, 36, 55, 123, 28, 20, 525, 33, 14, 114, 199, 204, 14, 19794, 8, 23750, 248, 508, 75, 27, 19, 195, 9, 7, 485, 8, 125, 365, 654, 75, 10, 16, 153, 1005, 14, 250, 14, 217, 11, 117, 16564, 9, 24, 7, 16, 210, 12, 906, 20, 160, 643, 160, 26, 7, 7460, 7, 19497, 10, 7, 3394, 7, 16850, 10, 7, 1519, 7, 8981, 38, 1030, 34, 12, 13539, 1414, 80, 1187, 4817, 12, 8665, 9, 7, 19, 31, 639, 4060, 12, 2814, 10, 48, 4443, 940, 10, 12, 69, 93, 2121, 196, 10, 27, 1329, 670, 10, 11, 12, 611, 14, 33891, 8, 1430, 11, 8, 0, 5104, 13, 62, 13528, 62, 9, 7, 30, 54, 15, 139, 1414, 14, 1116, 13, 10, 74, 458, 10, 74, 293, 89, 38, 169, 12, 81, 13, 134, 10, 69, 12, 81, 13, 3965, 11, 267, 672, 14, 615, 347, 229, 903, 9, 7, 1553, 43, 1152, 10, 63, 7, 3364, 7, 11471, 25, 8835, 3440, 32, 14, 1655, 27, 56, 10, 58, 32, 639, 122, 32, 323, 43, 493, 14, 6526, 34, 2862, 12, 10656, 1094, 66, 7, 11082, 1690, 8, 101, 466, 10, 214, 17, 5631, 13, 2908, 10, 11, 104, 64, 8, 651, 121, 38, 1744, 11, 3933, 14, 9, 7, 213, 28, 514, 8, 211, 20, 7, 3364, 11471, 22, 123, 14465, 8, 1894, 11, 1738, 23738, 13, 8358, 11, 2616, 14, 56, 4959, 134, 10, 261, 44, 89, 83, 15, 56, 278, 1329, 11, 30372, 42, 794, 10, 115, 1329, 11, 30372, 10, 27, 12, 977, 10, 8, 386, 9, 24, 7, 16, 22, 12, 768, 1063, 13, 5027, 14, 83, 12, 31, 27, 2158, 37, 436, 72, 14, 255, 752, 16, 22, 1048, 9, 18, 95, 951, 6070, 13, 8, 125, 1564, 10, 30, 16, 8911, 90, 14, 58, 52, 9, 7, 8, 31, 308, 67, 10, 8, 859, 95, 41, 98, 112, 103, 12, 1329, 31, 10, 30, 1088, 16, 25, 35, 14, 43, 9, 3], [2, 7, 448, 34, 12, 7, 1861, 7, 745, 680, 10, 42202, 200, 1595, 8, 4026, 11, 22074, 14, 406, 32, 17, 147, 1975, 9, 12, 1318, 0, 3140, 7, 10141, 33, 291, 14, 500, 11, 541, 492, 8, 110, 3444, 59, 3868, 9, 7, 95, 16, 43, 20, 8, 1798, 333, 61, 292, 72, 2072, 26, 48, 13185, 5142, 17, 12, 417, 500, 17, 7, 9863, 66, 3140, 7, 10141, 15, 4601, 11, 4053, 17, 19, 232, 20, 10066, 8, 281, 9, 7, 104, 1205, 38, 7, 1542, 7, 2469, 11, 7, 6117, 7, 24255, 9, 7, 2469, 15, 5787, 11, 7, 24255, 15, 5870, 13, 147, 686, 9, 7, 104, 17, 1418, 38, 13976, 7, 5232, 11, 7, 4458, 7, 9380, 9, 7, 37, 8, 138, 10, 918, 8, 269, 1425, 13, 7, 745, 22, 202, 34, 8, 281, 9, 3], [2, 18, 207, 19, 29, 28, 4940, 6943, 45, 12, 182, 60, 13, 914, 5242, 9, 18, 204, 77, 4940, 6943, 162, 9, 7, 96, 137, 10, 357, 246, 524, 10, 2369, 469, 10, 4465, 5935, 10, 76, 8, 496, 13, 8, 31, 15, 398, 9, 7, 16, 22, 37, 8, 857, 525, 20, 15322, 9, 18, 167, 42, 13, 8, 128, 49, 186, 785, 125, 11, 315, 776, 125, 70, 235, 9, 7, 8, 857, 525, 95, 41, 98, 1903, 69, 639, 10, 27, 94, 69, 686, 14, 1522, 11, 4979, 10, 11, 8, 621, 13, 1182, 636, 28, 8, 1456, 11, 79, 10462, 378, 11, 251, 15, 215, 14, 4986, 271, 9, 7, 104, 10, 54, 15, 139, 14, 74, 686, 1499, 14, 64, 95, 41, 2352, 307, 14, 899, 8, 1158, 1931, 20, 7, 1542, 7, 11545, 25, 7731, 17, 137, 60, 151, 509, 2956, 9, 24, 7, 599, 147, 75, 11, 6556, 9, 7, 58, 35, 126, 19, 398, 31, 9, 7, 63, 32, 1259, 16, 10, 32, 41, 77, 2413, 9, 7, 16, 22, 37, 12, 942, 475, 171, 92, 32, 78, 1367, 260, 8, 3567, 11, 114, 380, 13, 8, 3007, 439, 50, 3], [2, 7, 14, 3192, 7, 2784, 7, 29248, 36, 17, 8, 230, 7, 1354, 7, 2976, 2337, 21, 459, 66, 7, 109, 95, 200, 97, 120, 459, 66, 7, 213, 12, 185, 208, 149, 10, 7, 3957, 9, 7, 89, 198, 45, 8, 18022, 13, 584, 9, 21, 7, 139, 88, 7, 7546, 7, 1242, 140, 20, 39, 25, 5432, 8, 21, 705, 21, 14, 42, 13, 40, 138, 125, 9, 7, 1354, 7, 2976, 1356, 716, 12, 183, 324, 36, 55, 298, 16, 154, 43, 381, 33, 17, 96, 118, 9, 7, 77, 335, 1259, 19, 288, 549, 16, 73, 178, 43, 12, 705, 14, 8, 230, 10, 30, 89, 86, 4738, 694, 9, 7, 19, 31, 15, 872, 9, 7, 16, 3168, 74, 4232, 14, 8, 230, 10, 15, 48, 1548, 475, 13, 31, 10, 11, 48, 4664, 14, 8, 888, 67, 173, 49, 85, 8, 6591, 14, 43, 191, 13, 16, 9, 7, 16, 225, 41, 292, 12, 1920, 34, 946, 28, 8, 110, 96, 854, 9, 18, 84, 122, 89, 41, 12, 67, 439, 14, 1301, 12, 728, 212, 17474, 14, 7080, 8, 303, 89, 21798, 1030, 34, 19, 29, 9, 3], [2, 7, 19, 31, 395, 25, 357, 9, 18, 434, 14, 8, 1784, 1056, 158, 1117, 10, 11, 319, 25, 18087, 8, 1444, 14, 31876, 77, 21, 488, 8, 386, 130, 979, 180, 16, 570, 21, 1926, 9, 7, 639, 10, 8, 130, 25, 433, 27, 48, 1546, 4044, 24260, 52, 312, 95, 83, 9, 7, 177, 25, 395, 1585, 9, 7, 17, 211, 10, 76, 8, 613, 25, 1846, 27, 160, 619, 5631, 13, 2622, 20, 16, 84, 88, 35, 726, 72, 14, 94, 9, 24, 7, 8, 5062, 1615, 5673, 11, 1623, 695, 2072, 45, 8, 776, 156, 112, 16, 331, 44, 8, 69, 4880, 9, 7, 11, 8, 150, 135, 7, 7189, 22, 123, 15, 2729, 27, 8, 3820, 11, 3916, 40, 21, 16012, 16343, 21, 290, 14, 101, 8114, 14, 7111, 8, 648, 13, 8902, 882, 51, 8, 328, 23, 139, 9, 24, 7, 262, 8, 110, 644, 1269, 13, 8, 670, 25, 8, 21, 1717, 23, 7, 5011, 21, 2117, 99, 7, 190, 1279, 17, 8, 31, 10, 765, 16, 22, 8, 13900, 1279, 10, 55, 12, 345, 45, 9468, 10, 55, 12, 1520, 10, 25, 2074, 51, 12, 621, 13, 120, 10578, 1587, 36, 37, 14, 772, 12, 621, 13, 62, 5369, 62, 17, 8, 439, 13, 8, 1520, 33, 9, 18, 58, 35, 2298, 12, 690, 150, 68, 307, 14062, 34, 12, 646, 4694, 9, 7, 16, 274, 73, 41, 98, 358, 9, 24, 7, 583, 7, 7189, 84, 15, 35, 48, 7, 853, 23, 1229, 305, 10, 52, 18, 1371, 32, 41, 14, 213, 19, 31, 28, 64, 16, 22, 296, 9, 7, 17, 8, 148, 10, 18, 161, 4165, 8, 1203, 20, 8, 82, 168, 20, 73, 114, 19, 31, 331, 69, 437, 15, 14, 126, 16, 119, 169, 21, 7, 8, 7, 5744, 21, 9, 7, 888, 10, 32, 198, 337, 27, 48, 788, 20, 15, 782, 11, 4037, 10, 11, 1258, 752, 8, 3155, 13, 125, 13, 8, 503, 160, 26, 21, 7, 2973, 13, 8, 7, 8560, 21, 9, 3]...]\n",
       "Path: /Users/imad/.fastai/data/imdb\n",
       "y: ItemList (10066 items)\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n",
       "Path: /Users/imad/.fastai/data/imdb"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it all works ok: `x1`, `y1`, `x2` and `y2` should all be of size `bs`  by `bptt`. The texts in each row of `x1` should continue in `x2`. `y1` and `y2` should have the same texts as their `x` counterpart, shifted of one position to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl)\n",
    "x1, y1 = next(iter_dl)\n",
    "x2, y2 = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size(), y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = proc_num.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj first of all , this was n\\'t meant to be an \" xxmaj amityville \" sequel , it just tried to draw its audience by using that word in the title , but otherwise has little to do with what has happened before it in the series . xxmaj that would be ok if they had come up with a good flick anyway - but they did n\\'t'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj first of all , this was n\\'t meant to be an \" xxmaj amityville \" sequel , it just tried to draw its audience by using that word in the title , but otherwise has little to do with what has happened before it in the series . xxmaj that would be ok if they had come up with a good flick anyway - but they did n\\'t .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in y1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. xxmaj do you find it a good idea for a horror movie to give away its own surprises halfway through ? xxmaj if not , then avoid this one . ( * 1 / 2 ) xxeos xxbos xxmaj what happens when an army of xxunk , xxunk , and xxmaj godless xxmaj eastern xxmaj european commies gather their forces south of the border ? xxmaj gary xxmaj busey'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's prepare some convenience function to do this quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "    return (\n",
    "        DataLoader(\n",
    "            LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs\n",
    "        ),\n",
    "        DataLoader(\n",
    "            LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2 * bs, **kwargs\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def lm_databunchify(sd, bs, bptt, **kwargs):\n",
    "    return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 64, 70\n",
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Batching for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we will want to tackle classification, gathering the data will be a bit different: first we will label our texts with the folder they come from, and then we will need to apply padding to batch them together. To avoid mixing very long texts with very short ones, we will also use `Sampler` to sort (with a bit of randomness for the training set) our samples by length.\n",
    "\n",
    "First the data block API calls shold look familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 12 video](https://course.fast.ai/videos/?lesson=12&t=5877)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_cat = CategoryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:11<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:12<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "il = TextList.from_files(path, include=[\"train\", \"test\"])\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=\"test\"))\n",
    "ll = label_by_func(sd, parent_labeler, proc_x=[proc_tok, proc_num], proc_y=proc_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path / \"ll_clas.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path / \"ll_clas.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the labels seem consistent with the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(ll.train.x_obj(i), ll.train.y_obj(i)) for i in [1,12552]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw samplers in notebook 03. For the validation set, we will simply sort the samples by length, and we begin with the longest ones for memory reasons (it's better to always have the biggest tensors first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "# Sort without randomness -> sort solely based on the length of doc in descending\n",
    "class SortSampler(Sampler):\n",
    "    def __init__(self, data_source, key):\n",
    "        self.data_source, self.key = data_source, key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(\n",
    "            sorted(list(range(len(self.data_source))), key=self.key, reverse=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set, we want some kind of randomness on top of this. So first, we shuffle the texts and build megabatches of size `50 * bs`. We sort those megabatches by length before splitting them in 50 minibatches. That way we will have randomized batches of roughly the same length.\n",
    "\n",
    "Then we make sure to have the biggest batch first and shuffle the order of the other batches. We also make sure the last batch stays at the end because its size is probably lower than batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SortishSampler(Sampler):\n",
    "    def __init__(self, data_source, key, bs):\n",
    "        self.data_source, self.key, self.bs = data_source, key, bs\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Get random indices\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [\n",
    "            idxs[i : i + self.bs * 50] for i in range(0, len(idxs), self.bs * 50)\n",
    "        ]\n",
    "        sorted_idx = torch.cat(\n",
    "            [tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches]\n",
    "        )\n",
    "        batches = [\n",
    "            sorted_idx[i : i + self.bs] for i in range(0, len(sorted_idx), self.bs)\n",
    "        ]\n",
    "        max_idx = torch.argmax(\n",
    "            tensor([self.key(ck[0]) for ck in batches])\n",
    "        )  # find the chunk with the largest key,\n",
    "        batches[0], batches[max_idx] = (\n",
    "            batches[max_idx],\n",
    "            batches[0],\n",
    "        )  # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches) - 2)\n",
    "        sorted_idx = (\n",
    "            torch.cat([batches[i + 1] for i in batch_idxs])\n",
    "            if len(batches) > 1\n",
    "            else LongTensor([])\n",
    "        )\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding: we had the padding token (that as an id of 1) at the end of each sequence to make them all the same size when batching them. Note that we need padding at the end to be able to use `PyTorch` convenience functions that will let us ignore that padding (see 12c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i, s in enumerate(samples):\n",
    "        if pad_first:\n",
    "            res[i, -len(s[0]) :] = LongTensor(s[0])\n",
    "        else:\n",
    "            res[i, : len(s[0])] = LongTensor(s[0])\n",
    "    return res, tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "train_sampler = SortishSampler(\n",
    "    ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs\n",
    ")\n",
    "train_dl = DataLoader(\n",
    "    ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-409e9ea36e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miter_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastbook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/courses/fastai-courses/dl2/notebooks/exp/nb_08.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/courses/fastai-courses/dl2/notebooks/exp/nb_08.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/courses/fastai-courses/dl2/notebooks/exp/nb_06.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bool mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "iter_dl = iter(train_dl)\n",
    "x,y = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one is the minimal length. This is the first batch so it has the longest sequence, but if look at the next one that is more random, we see lengths are roughly the sames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter_dl)\n",
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the padding at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-6fcf9dfbd479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we add a convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n",
    "    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "    return (\n",
    "        DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=bs,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=pad_collate,\n",
    "            **kwargs\n",
    "        ),\n",
    "        DataLoader(\n",
    "            valid_ds,\n",
    "            batch_size=bs * 2,\n",
    "            sampler=valid_sampler,\n",
    "            collate_fn=pad_collate,\n",
    "            **kwargs\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def clas_databunchify(sd, bs, **kwargs):\n",
    "    return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 64, 70\n",
    "data = clas_databunchify(ll, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12_text.ipynb to exp/nb_12.py\n"
     ]
    }
   ],
   "source": [
    "!python ../src/notebook2script.py 12_text.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
